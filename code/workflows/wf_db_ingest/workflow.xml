<workflow-app xmlns='uri:oozie:workflow:0.5' name='wf_oozie_${env}_${app}_${table}_ingest'>
	<credentials>
		<credential name='hive_credentials' type='hive2'>
			<property>
				<name>hive2.jdbc.url</name>
				<value>${hive2JDBC}</value>
			</property>
			<property>
				<name>hive2.server.principal</name>
				<value>${hive2Principal}</value>
			</property>
		</credential>
	</credentials>
	<start to='SqoopDB'/>
	<!-- Step 1: Sqoop data -->
	<action name='SqoopDB'>
		<sqoop xmlns="uri:oozie:sqoop-action:0.3">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
						<prepare>
				<delete path='${stg_target_dir}'/>
			</prepare>
			<configuration>
				<property>
					<name>hadoop.security.credential.provider.path</name>
					<value>${password_key_provider}</value>
				</property>
				<property>
					<name>oracle.jdbc.mapDateToTimestamp</name>
					<value>false</value>
				</property>
				<property>
					<name>oozie.launcher.mapred.map.child.env</name>
					<value>HADOOP_CREDSTORE_PASSWORD=none</value>
				</property>
				<!--property>
					<name>org.apache.sqoop.credentials.loader.class</name>
					<value>org.apache.sqoop.util.password.CredentialProviderPasswordLoader</value>
				</property-->
				<property>
					<name>mapreduce.job.queuename</name>
					<value>${default_queuename}</value>
				</property>
			</configuration>
			<arg>import</arg>
		  <arg>-Dhadoop.security.credential.provider.path=${password_key_provider}</arg>
			<arg>--options-file</arg><arg>${datasource}_options_file.sqoop</arg>
			<arg>--connect</arg><arg>${jdbc_connect}</arg>
			<arg>--username</arg><arg>${username}</arg>
			<!--arg>-password-file</arg><arg>/user/mrice/mfspw.txt</arg-->
			<arg>--password-alias</arg><arg>${password_alias}</arg>
			<arg>--table</arg><arg>${table}</arg>
			<arg>--columns</arg><arg>${columns}</arg>
			<arg>--where</arg><arg>${where}</arg>
			<arg>--verbose</arg>
			<arg>--m</arg><arg>${num_mappers}</arg>
			<arg>--split-by</arg><arg>${split_by_column}</arg>
			<arg>--fields-terminated-by</arg><arg>${fields_terminated_by}</arg>
			<arg>--null-string</arg><arg>${null_string}</arg>
			<arg>--null-non-string</arg><arg>${null_non_string}</arg>
			<arg>--hive-delims-replacement</arg><arg>${delim_replace}</arg>
			<arg>--map-column-java</arg><arg>${map_column_java}</arg>
			<!--arg>-connections-parm-file</arg><arg>${datasource}_connection.properties</arg-->

			<arg>--target-dir</arg><arg>${stg_target_dir}</arg>
			<file>${datasource}_connection.properties</file>
			<file>${datasource}_options_file.sqoop</file>
		</sqoop>
		<ok to="hdfscommands"/>
		<error to="email_failure"/>
	</action>

	<!--Step2: Step sqooped file permisions to 777 -->
	<action name="hdfscommands">
		<fs>
			<chmod path='${stg_target_dir}' permissions='-rwxrwx--x' dir-files='true'><recursive/></chmod>
		</fs>
		<ok to="RunPreValidations"/>
		<error to="email_failure"/>
	</action>
	<!--Step 3: Run validations python script -->
	<action name="RunPreValidations">
		<shell xmlns="uri:oozie:shell-action:0.1">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<exec>./run_validations.py</exec>
			<argument>--user</argument><argument>${oozie_user_name}</argument>
			<argument>--domain</argument><argument>${domainName}</argument>
			<argument>--keystore</argument><argument>${hdfs_service_keystore_root}</argument>
			<argument>--hv_table</argument><argument>${hv_table}</argument>
			<argument>--hv_db</argument><argument>${hv_db}</argument>
			<argument>--hv_db_stage</argument><argument>${hv_db_stage}</argument>
			<argument>--stage_table</argument><argument>${stage_table}</argument>
			<argument>--validation_type</argument><argument>${pre_validation_type}</argument>
			<argument>--impalaConnect</argument><argument>${impalaConnect}</argument>
			<argument>--where_hadoop</argument><argument>${where_hadoop}</argument>
			<argument>--jdbc_connect</argument><argument>${jdbc_connect}</argument>
			<argument>--table</argument><argument>${table}</argument>
			<argument>--where</argument><argument>${where}</argument>
			<argument>--username</argument><argument>${username}</argument>
			<argument>--datasource</argument><argument>${datasource}</argument>
			<env-var>PATH=/opt/cloudera/parcels/Anaconda/bin:/sbin:/usr/sbin:/bin:/usr/bin</env-var>
			<env-var>PYTHON_EGG_CACHE=/tmp</env-var>
			<env-var>HADOOP_CLASSPATH=${HADOOP_CLASSPATH}</env-var> 
			<file>run_validations.py#run_validations.py</file>
		</shell>
		<ok to="HiveStgToFinal"/>
		<error to="email_failure"/>
	</action>
	<!--Step4: Insert into final table using Hive -->
	<action name="HiveStgToFinal" cred="hive_credentials">
		<hive2 xmlns="uri:oozie:hive2-action:0.1">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<property>
					<name>mapreduce.job.queuename</name>
					<value>${default_queuename}</value>
				</property>
			</configuration>
			<jdbc-url>${hive2JDBC}</jdbc-url>
			<script>${hive_query}</script>
			<param>hv_db_stage=${hv_db_stage}</param>
			<param>hv_db=${hv_db}</param>
			<param>table=${table}</param>
			<param>hv_table=${hv_table}</param>
			<param>partition_clause=${partition_clause}</param>
			<param>columns_without_partition=${hv_columns_without_partition}</param>
			<param>partition_column_select=${partition_column_select}</param>
			<param>where_column=${where_column}</param>
			<param>partition_column=${partition_column}</param>
			<param>hdfs_load_ts=${hdfs_load_ts}</param>
			<param>stage_table=${stage_table}</param>
			<param>default_queuename=${default_queuename}</param>
		</hive2>
		<ok to="RunValidations" />
		<error to="email_failure" />
	</action>
	<!--Step 5: Run validations python script -->
	<action name="RunValidations">
		<shell xmlns="uri:oozie:shell-action:0.1">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<exec>./run_validations.py</exec>
			<argument>--user</argument><argument>${oozie_user_name}</argument>
			<argument>--domain</argument><argument>${domainName}</argument>
			<argument>--keystore</argument><argument>${hdfs_service_keystore_root}</argument>
			<argument>--hv_table</argument><argument>${hv_table}</argument>
			<argument>--hv_db</argument><argument>${hv_db}</argument>
			<argument>--hv_db_stage</argument><argument>${hv_db_stage}</argument>
			<argument>--stage_table</argument><argument>${stage_table}</argument>
			<argument>--validation_type</argument><argument>${validation_type}</argument>
			<argument>--impalaConnect</argument><argument>${impalaConnect}</argument>
			<argument>--where_hadoop</argument><argument>${where_hadoop}</argument>
			<argument>--jdbc_connect</argument><argument>${jdbc_connect}</argument>
			<argument>--table</argument><argument>${table}</argument>
			<argument>--where</argument><argument>${where}</argument>
			<argument>--username</argument><argument>${username}</argument>
			<argument>--datasource</argument><argument>${datasource}</argument>
			<env-var>PATH=/opt/cloudera/parcels/Anaconda/bin:/sbin:/usr/sbin:/bin:/usr/bin</env-var>
			<env-var>PYTHON_EGG_CACHE=/tmp</env-var>
			<env-var>HADOOP_CLASSPATH=${HADOOP_CLASSPATH}</env-var>
			<file>run_validations.py#run_validations.py</file> 
		</shell>
		<ok to="end"/>
		<error to="email_failure"/>
	</action>
	<!--Step 6: Send Failure email -->
	<action name="email_failure">
		<email xmlns="uri:oozie:email-action:0.1">
			<to>${email_list}</to>
			<subject>OOZIE: ${app} Ingest Job Failed </subject>
			<body>
There is error in the wf_oozie_${env}_${app}_${table}_ingest workflow. 
	APP			: ${app}
	SUB_APP		: ${sub_app}
	Stage Table		: ${hv_db_stage}.${stage_table}
	Final Table		: ${hv_db}.${hv_table}
	Failed Node		: ${wf:lastErrorNode()}
			
Please refer to below for more information:
	1. OOZIE workflow Dashboard: ${hueNode}/oozie/list_oozie_workflow/${wf:id()}
	2. EdgeNodes Log-File: ${log_file} 

Re-Run Instructions:
	1. Log on to Edgenodes 
	2. Trigger following Command:
		python /cloudera_nfs1/code/scripts/run_job.py ${group} ${app} ${sub_app} 
Note: Before rerunning make sure if re-run is absolutely necessary.
			</body>
		</email>
		<ok to="fail"/>
		<error to="fail"/>
	</action>
	<kill name="fail">
		<message>Sqoop failed, error messafe[${wf:errorMessage(wf:lastErrorNode())}]</message>
	</kill>
	<end name='end'/>
</workflow-app> 
					
